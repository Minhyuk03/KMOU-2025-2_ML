{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA5Pw6t/4oq7CPgCwX31wy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minhyuk03/KMOU_2-2_ML/blob/main/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5_8%EC%A3%BC%EC%B0%A8_7%EA%B0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB 7-1 AND/OR 연산을 수행하는 퍼셉트론"
      ],
      "metadata": {
        "id": "818r0f6XL5wm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05cfddb1",
        "outputId": "5d40cc6c-fc7e-4f7a-ae9c-4c12f7d18d1d"
      },
      "source": [
        "def activation_function(x):\n",
        "    return 1 if x > 0 else -1\n",
        "\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    weighted_sum = x1 * w1 + x2 * w2 + b\n",
        "    return activation_function(weighted_sum)\n",
        "\n",
        "# AND 연산을 위한 퍼셉트론\n",
        "# 적절한 가중치(w1, w2)와 편향(b)을 설정\n",
        "w1_and, w2_and, b_and = 0.5, 0.5, -0.7\n",
        "print(\"LAB 7-1\\n\")\n",
        "print(\"AND 연산 결과:\")\n",
        "print(f\"입력 ( 1,  1): {perceptron(1, 1, w1_and, w2_and, b_and)}\")\n",
        "print(f\"입력 ( 1, -1): {perceptron(1, -1, w1_and, w2_and, b_and)}\")\n",
        "print(f\"입력 (-1,  1): {perceptron(-1, 1, w1_and, w2_and, b_and)}\")\n",
        "print(f\"입력 (-1, -1): {perceptron(-1, -1, w1_and, w2_and, b_and)}\")\n",
        "\n",
        "# OR 연산을 위한 퍼셉트론\n",
        "# 적절한 가중치(w1, w2)와 편향(b)을 설정\n",
        "w1_or, w2_or, b_or = 0.5, 0.5, 0.2\n",
        "\n",
        "print(\"\\nOR 연산 결과:\")\n",
        "print(f\"입력 ( 1,  1): {perceptron(1, 1, w1_or, w2_or, b_or)}\")\n",
        "print(f\"입력 ( 1, -1): {perceptron(1, -1, w1_or, w2_or, b_or)}\")\n",
        "print(f\"입력 (-1,  1): {perceptron(-1, 1, w1_or, w2_or, b_or)}\")\n",
        "print(f\"입력 (-1, -1): {perceptron(-1, -1, w1_or, w2_or, b_or)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAB 7-1\n",
            "\n",
            "AND 연산 결과:\n",
            "입력 ( 1,  1): 1\n",
            "입력 ( 1, -1): -1\n",
            "입력 (-1,  1): -1\n",
            "입력 (-1, -1): -1\n",
            "\n",
            "OR 연산 결과:\n",
            "입력 ( 1,  1): 1\n",
            "입력 ( 1, -1): 1\n",
            "입력 (-1,  1): 1\n",
            "입력 (-1, -1): -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB 7-2"
      ],
      "metadata": {
        "id": "qWuXXvVJMo9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"LAB 7-2\\n\")\n",
        "# XOR 데이터셋 정의\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "\n",
        "# 가중치와 편향 초기화\n",
        "W = np.zeros(X.shape[1]) # 입력 특성의 수만큼 가중치 초기화\n",
        "b = 0\n",
        "\n",
        "# 학습률 (learning rate)\n",
        "lr = 0.1\n",
        "\n",
        "# 학습 횟수 (epochs)\n",
        "epochs = 100\n",
        "\n",
        "# 퍼셉트론 학습\n",
        "for epoch in range(epochs):\n",
        "    for xi, target in zip(X, y):\n",
        "        linear_output = np.dot(W, xi) + b\n",
        "        predicted_output = 1 if linear_output >= 0 else -1\n",
        "\n",
        "        # 오차 계산\n",
        "        error = target - predicted_output\n",
        "\n",
        "        # 가중치와 편향 업데이트\n",
        "        if error != 0:\n",
        "            update = lr * target\n",
        "\n",
        "            W += update * xi\n",
        "            b += update\n",
        "\n",
        "print(\"학습된 W:\", W)\n",
        "print(\"학습된 b:\", b)\n",
        "\n",
        "# 학습된 퍼셉트론으로 예측\n",
        "print(\"\\n학습된 퍼셉트론으로 예측:\")\n",
        "for xi, target in zip(X, y):\n",
        "     linear_output = np.dot(W, xi) + b\n",
        "     y_pred = 1 if linear_output >= 0 else -1\n",
        "     print(f\"입력: {xi}, 실제 타겟: {target}, 예측 결과: {y_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwcKIN3zDq2f",
        "outputId": "99998029-3f9a-4d3a-9ce9-8adeeeb42635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAB 7-2\n",
            "\n",
            "학습된 W: [0. 0.]\n",
            "학습된 b: 0.0\n",
            "\n",
            "학습된 퍼셉트론으로 예측:\n",
            "입력: [0 0], 실제 타겟: 0, 예측 결과: 1\n",
            "입력: [0 1], 실제 타겟: 1, 예측 결과: 1\n",
            "입력: [1 0], 실제 타겟: 1, 예측 결과: 1\n",
            "입력: [1 1], 실제 타겟: 0, 예측 결과: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB 7-3"
      ],
      "metadata": {
        "id": "1cw4QC1EMsjm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32d114d9",
        "outputId": "04989f7c-68f2-4989-c15b-40e07204c8e4"
      },
      "source": [
        "import numpy as np\n",
        "print(\"LAB 7-3\\n\")\n",
        "def activation_function(x):\n",
        "    return 1 if x > 0 else -1\n",
        "\n",
        "def train_perceptron(X, y, learning_rate=0.1, epochs=100):\n",
        "    # 가중치와 편향 초기화\n",
        "    W = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for xi, target in zip(X, y):\n",
        "            # 선형 결합 계산: W * x + b\n",
        "            linear_output = np.dot(W, xi) + b\n",
        "            # 예측 결과 (활성화 함수 적용)\n",
        "            predicted_output = activation_function(linear_output)\n",
        "            # 오차 계산\n",
        "            error = target - predicted_output\n",
        "            # 가중치와 편향 업데이트\n",
        "            if error != 0:\n",
        "                update = learning_rate * target\n",
        "\n",
        "                W += update * xi\n",
        "                b += update\n",
        "\n",
        "    return W, b\n",
        "\n",
        "def predict_perceptron(X, W, b):\n",
        "    predictions = []\n",
        "    for xi in X:\n",
        "        linear_output = np.dot(W, xi) + b\n",
        "        predictions.append(activation_function(linear_output))\n",
        "    return np.array(predictions)\n",
        "\n",
        "\n",
        "# OR 연산 학습 예시\n",
        "print(\"OR 연산 학습\")\n",
        "X_or = np.array([[1, 1],\n",
        "                 [1, -1],\n",
        "                 [-1, 1],\n",
        "                 [-1, -1]])\n",
        "y_or = np.array([1, 1, 1, -1])\n",
        "\n",
        "W_or, b_or = train_perceptron(X_or, y_or)\n",
        "\n",
        "print(\"\\n학습된 OR 퍼셉트론으로 예측:\")\n",
        "predictions_or = predict_perceptron(X_or, W_or, b_or)\n",
        "print(f\"입력:\\n{X_or}\")\n",
        "print(f\"예측 결과: {predictions_or}\")\n",
        "print(f\"실제 타겟:   {y_or}\")\n",
        "\n",
        "# AND 연산 학습 예시\n",
        "print(\"\\nAND 연산 학습\")\n",
        "X_and = np.array([[1, 1],\n",
        "                  [1, -1],\n",
        "                  [-1, 1],\n",
        "                  [-1, -1]])\n",
        "y_and = np.array([1, -1, -1, -1])\n",
        "\n",
        "W_and, b_and = train_perceptron(X_and, y_and)\n",
        "\n",
        "print(\"\\n학습된 AND 퍼셉트론으로 예측:\")\n",
        "predictions_and = predict_perceptron(X_and, W_and, b_and)\n",
        "print(f\"입력:\\n{X_and}\")\n",
        "print(f\"예측 결과: {predictions_and}\")\n",
        "print(f\"실제 타겟:   {y_and}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAB 7-3\n",
            "\n",
            "OR 연산 학습\n",
            "\n",
            "학습된 OR 퍼셉트론으로 예측:\n",
            "입력:\n",
            "[[ 1  1]\n",
            " [ 1 -1]\n",
            " [-1  1]\n",
            " [-1 -1]]\n",
            "예측 결과: [ 1  1  1 -1]\n",
            "실제 타겟:   [ 1  1  1 -1]\n",
            "\n",
            "AND 연산 학습\n",
            "\n",
            "학습된 AND 퍼셉트론으로 예측:\n",
            "입력:\n",
            "[[ 1  1]\n",
            " [ 1 -1]\n",
            " [-1  1]\n",
            " [-1 -1]]\n",
            "예측 결과: [ 1 -1 -1 -1]\n",
            "실제 타겟:   [ 1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB 7-4"
      ],
      "metadata": {
        "id": "uk0IsPv9MuNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"LAB 7-4\\n\")\n",
        "# XOR 데이터셋 정의\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "def polynomial_transform(X):\n",
        "    x1 = X[:, 0]\n",
        "    x2 = X[:, 1]\n",
        "    x1x2 = x1 * x2\n",
        "    ones = np.ones(X.shape[0])\n",
        "\n",
        "    X_poly = np.column_stack([x1, x2, x1x2, ones])\n",
        "    return X_poly\n",
        "\n",
        "# 다항 변환 적용\n",
        "X_poly = polynomial_transform(X)\n",
        "\n",
        "print(\"원본 입력:\")\n",
        "print(X)\n",
        "print(\"\\n다항 변환 후 특징 (x1, x2, x1*x2, 1):\")\n",
        "print(X_poly)\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, n_epochs=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.weights = None\n",
        "        self.training_errors = []\n",
        "\n",
        "    def activation(self, z):\n",
        "        return np.where(z >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        # 학습 반복\n",
        "        for epoch in range(self.n_epochs):\n",
        "            errors = 0\n",
        "\n",
        "            for xi, target in zip(X, y):\n",
        "\n",
        "                z = np.dot(xi, self.weights)\n",
        "                prediction = self.activation(z)\n",
        "\n",
        "                update = self.learning_rate * (target - prediction)\n",
        "                self.weights += update * xi\n",
        "\n",
        "                errors += int(update != 0.0)\n",
        "\n",
        "            self.training_errors.append(errors)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.weights)\n",
        "        return self.activation(z)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        return np.mean(predictions == y)\n",
        "\n",
        "\n",
        "perceptron = Perceptron(learning_rate=0.1, n_epochs=1000)\n",
        "perceptron.fit(X_poly, y)\n",
        "\n",
        "# 학습된 가중치 출력\n",
        "print(f\"\\n학습된 가중치: {perceptron.weights}\")\n",
        "print(f\"w1={perceptron.weights[0]:.2f}, w2={perceptron.weights[1]:.2f}, \"\n",
        "      f\"w3={perceptron.weights[2]:.2f}, bias={perceptron.weights[3]:.2f}\")\n",
        "\n",
        "# 예측 결과 확인\n",
        "predictions = perceptron.predict(X_poly)\n",
        "\n",
        "print(\"입력 (x1, x2) | 실제값 | 예측값\")\n",
        "for i in range(len(X)):\n",
        "    x1, x2 = X[i]\n",
        "    actual = y[i]\n",
        "    pred = predictions[i]\n",
        "    print(f\"({x1},{x2}) | {x1} XOR {x2} = {actual} | {pred}\")\n"
      ],
      "metadata": {
        "id": "sGV052RLMyre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122f28c9-6df5-43af-ea63-7e06f72e4c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAB 7-4\n",
            "\n",
            "원본 입력:\n",
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "\n",
            "다항 변환 후 특징 (x1, x2, x1*x2, 1):\n",
            "[[0. 0. 0. 1.]\n",
            " [0. 1. 0. 1.]\n",
            " [1. 0. 0. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "\n",
            "학습된 가중치: [ 0.1  0.1 -0.3 -0.1]\n",
            "w1=0.10, w2=0.10, w3=-0.30, bias=-0.10\n",
            "입력 (x1, x2) | 실제값 | 예측값\n",
            "(0,0) | 0 XOR 0 = 0 | 0\n",
            "(0,1) | 0 XOR 1 = 1 | 1\n",
            "(1,0) | 1 XOR 0 = 1 | 1\n",
            "(1,1) | 1 XOR 1 = 0 | 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텐서플로우를 이용한 MLP 구현"
      ],
      "metadata": {
        "id": "W2IuripCMwbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "print(\"TensorFlow MLP\\n\")\n",
        "# 데이터 준비\n",
        "X_train = np.array([[0, 0],\n",
        "                    [0, 1],\n",
        "                    [1, 0],\n",
        "                    [1, 1]], dtype=np.float32)\n",
        "\n",
        "y_train = np.array([0, 1, 1, 0])\n",
        "\n",
        "print(\"scikit-learn MLPClassifier를 이용한 XOR 연산\")\n",
        "\n",
        "# MLP 모델 생성\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(4, 4),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    learning_rate_init=0.1,\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n모델 구조:\")\n",
        "print(\"입력층: 2 뉴런\")\n",
        "print(\"은닉층1: 4 뉴런 (ReLU)\")\n",
        "print(\"은닉층2: 4 뉴런 (ReLU)\")\n",
        "print(\"출력층: 1 뉴런 (Sigmoid)\")\n",
        "\n",
        "# 모델 학습\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "predictions = mlp.predict(X_train)\n",
        "probabilities = mlp.predict_proba(X_train)\n",
        "\n",
        "# 평가\n",
        "accuracy = accuracy_score(y_train, predictions)\n",
        "\n",
        "print(\"\\n예측 결과:\")\n",
        "print(\"입력 (x1, x2) | 실제값 | 예측값 | 확률값 (클래스 1)\")\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    x1, x2 = X_train[i]\n",
        "    actual = y_train[i]\n",
        "    pred = predictions[i]\n",
        "    prob = probabilities[i][1]\n",
        "    print(f\"({int(x1)}, {int(x2)}) | {int(actual)} | {pred} | {prob:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qR3GQENXMy98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff530847-e257-4169-c392-5cec1f5728d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow MLP\n",
            "\n",
            "scikit-learn MLPClassifier를 이용한 XOR 연산\n",
            "\n",
            "모델 구조:\n",
            "입력층: 2 뉴런\n",
            "은닉층1: 4 뉴런 (ReLU)\n",
            "은닉층2: 4 뉴런 (ReLU)\n",
            "출력층: 1 뉴런 (Sigmoid)\n",
            "Iteration 1, loss = 0.72561148\n",
            "Iteration 2, loss = 0.70392069\n",
            "Iteration 3, loss = 0.67612176\n",
            "Iteration 4, loss = 0.65579365\n",
            "Iteration 5, loss = 0.62459002\n",
            "Iteration 6, loss = 0.59290886\n",
            "Iteration 7, loss = 0.57682609\n",
            "Iteration 8, loss = 0.53816946\n",
            "Iteration 9, loss = 0.50723096\n",
            "Iteration 10, loss = 0.46672108\n",
            "Iteration 11, loss = 0.42595202\n",
            "Iteration 12, loss = 0.37906117\n",
            "Iteration 13, loss = 0.35478724\n",
            "Iteration 14, loss = 0.32762777\n",
            "Iteration 15, loss = 0.29507610\n",
            "Iteration 16, loss = 0.27081287\n",
            "Iteration 17, loss = 0.24701308\n",
            "Iteration 18, loss = 0.21490758\n",
            "Iteration 19, loss = 0.19520710\n",
            "Iteration 20, loss = 0.17527288\n",
            "Iteration 21, loss = 0.15650302\n",
            "Iteration 22, loss = 0.13974893\n",
            "Iteration 23, loss = 0.12546505\n",
            "Iteration 24, loss = 0.11335049\n",
            "Iteration 25, loss = 0.10292080\n",
            "Iteration 26, loss = 0.09820817\n",
            "Iteration 27, loss = 0.08607683\n",
            "Iteration 28, loss = 0.11898984\n",
            "Iteration 29, loss = 0.07175513\n",
            "Iteration 30, loss = 0.06520604\n",
            "Iteration 31, loss = 0.05935547\n",
            "Iteration 32, loss = 0.05411844\n",
            "Iteration 33, loss = 0.04941648\n",
            "Iteration 34, loss = 0.04520849\n",
            "Iteration 35, loss = 0.04149267\n",
            "Iteration 36, loss = 0.03821622\n",
            "Iteration 37, loss = 0.03534659\n",
            "Iteration 38, loss = 0.03285068\n",
            "Iteration 39, loss = 0.03069157\n",
            "Iteration 40, loss = 0.02882471\n",
            "Iteration 41, loss = 0.02719553\n",
            "Iteration 42, loss = 0.02574108\n",
            "Iteration 43, loss = 0.02439655\n",
            "Iteration 44, loss = 0.02310722\n",
            "Iteration 45, loss = 0.02183981\n",
            "Iteration 46, loss = 0.02058779\n",
            "Iteration 47, loss = 0.01936611\n",
            "Iteration 48, loss = 0.01819989\n",
            "Iteration 49, loss = 0.01711257\n",
            "Iteration 50, loss = 0.01611899\n",
            "Iteration 51, loss = 0.01522386\n",
            "Iteration 52, loss = 0.01442388\n",
            "Iteration 53, loss = 0.01371098\n",
            "Iteration 54, loss = 0.01307508\n",
            "Iteration 55, loss = 0.01250599\n",
            "Iteration 56, loss = 0.01199419\n",
            "Iteration 57, loss = 0.01153150\n",
            "Iteration 58, loss = 0.01111477\n",
            "Iteration 59, loss = 0.01074312\n",
            "Iteration 60, loss = 0.01039911\n",
            "Iteration 61, loss = 0.01007997\n",
            "Iteration 62, loss = 0.00978319\n",
            "Iteration 63, loss = 0.00950651\n",
            "Iteration 64, loss = 0.00924806\n",
            "Iteration 65, loss = 0.00900610\n",
            "Iteration 66, loss = 0.00877912\n",
            "Iteration 67, loss = 0.00856574\n",
            "Iteration 68, loss = 0.00836483\n",
            "Iteration 69, loss = 0.00817527\n",
            "Iteration 70, loss = 0.00799611\n",
            "Iteration 71, loss = 0.00782651\n",
            "Iteration 72, loss = 0.00766566\n",
            "Iteration 73, loss = 0.00751295\n",
            "Iteration 74, loss = 0.00736767\n",
            "Iteration 75, loss = 0.00722929\n",
            "Iteration 76, loss = 0.00709730\n",
            "Iteration 77, loss = 0.00697121\n",
            "Iteration 78, loss = 0.00685059\n",
            "Iteration 79, loss = 0.00673510\n",
            "Iteration 80, loss = 0.00662444\n",
            "Iteration 81, loss = 0.00651817\n",
            "Iteration 82, loss = 0.00641606\n",
            "Iteration 83, loss = 0.00631783\n",
            "Iteration 84, loss = 0.00622326\n",
            "Iteration 85, loss = 0.00613209\n",
            "Iteration 86, loss = 0.00604410\n",
            "Iteration 87, loss = 0.00595919\n",
            "Iteration 88, loss = 0.00587707\n",
            "Iteration 89, loss = 0.00579767\n",
            "Iteration 90, loss = 0.00572080\n",
            "Iteration 91, loss = 0.00564634\n",
            "Iteration 92, loss = 0.00557413\n",
            "Iteration 93, loss = 0.00550410\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "\n",
            "예측 결과:\n",
            "입력 (x1, x2) | 실제값 | 예측값 | 확률값 (클래스 1)\n",
            "(0, 0) | 0 | 0 | 0.0000\n",
            "(0, 1) | 1 | 1 | 0.9998\n",
            "(1, 0) | 1 | 1 | 0.9997\n",
            "(1, 1) | 0 | 0 | 0.0178\n"
          ]
        }
      ]
    }
  ]
}